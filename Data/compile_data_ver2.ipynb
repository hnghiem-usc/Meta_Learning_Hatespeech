{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d59aa5-6efc-41db-9536-687aac79e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VER 2: EDIT DATA COLLECTOR CLASS TO ENABLE SWITCHING BETWEEN BINARY AND ORIG MODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa659330-d475-438d-be93-c3bbd246bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//Users/hn/Desktop/COVID_Racism/env/lib/python3.8/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random, json, gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from math import ceil, floor\n",
    "\n",
    "sys.path.append(\"../Code/utils/\")\n",
    "from text_processing import process_tweet_bert\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# segmenter using the word statistics from english Wikipedia\n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "seg_tw = Segmenter(corpus=\"twitter\") \n",
    "\n",
    "def fill_text_na(df:pd.DataFrame, cols: list = ['text_std'], fill_value=' '):\n",
    "    \"\"\"\n",
    "        Fill text variables with space for tokenizer to work\n",
    "    \"\"\"\n",
    "    dt = df.copy()\n",
    "    dt[cols] =  df[cols].fillna(fill_value)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3b19aa-de3a-4f1b-8135-f77ec660527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_sample(df:pd.DataFrame, var_to_sample:'str', K:int, seed=123):\n",
    "    \"\"\"\n",
    "        Sample a data with as equal of the var_to_sample as much as possible \n",
    "    \"\"\"\n",
    "    if df.shape[0] <= K: \n",
    "        # if not enough to meet quota, return dataset\n",
    "        return df\n",
    "    elif df[var_to_sample].nunique == 1 : # only 1 class\n",
    "        return df.sample(K, random_state=seed)\n",
    "    else: \n",
    "        counter_dict =  dict(Counter(df[var_to_sample]))\n",
    "        ideal_cat_size = int(K/len(counter_dict))\n",
    "\n",
    "        delta_dict = {k: v - ideal_cat_size for k,v in counter_dict.items()}\n",
    "        delta_dict = {k: v for k, v in sorted(delta_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "        k_needed = K \n",
    "        num_class_elig = len(counter_dict)\n",
    "\n",
    "        append_ls = []\n",
    "        for c, v in delta_dict.items(): \n",
    "            extra_needed = counter_dict[c]\n",
    "            if v > 0: \n",
    "                extra_needed = floor(k_needed / num_class_elig)\n",
    "            n_in_sample = min(counter_dict[c],  extra_needed)\n",
    "            append_ls.append(df[df[var_to_sample] == c].sample(n_in_sample, random_state=seed))\n",
    "\n",
    "            # update parameters\n",
    "            num_class_elig -= 1\n",
    "            k_needed -= n_in_sample\n",
    "        \n",
    "        return pd.concat(append_ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f0e743-167c-4714-98c7-8c312b026a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_label(f, var:str):\n",
    "    d_t = np.array(f[var].apply(lambda x: [int(y) for y in x[1:-1].split(',')]).values.tolist())\n",
    "    num_cols = d_t.shape[-1]\n",
    "    columns=['v'+str(i) for i in range(num_cols)]\n",
    "    split_dt =   pd.DataFrame(np.array(d_t), columns=columns)\n",
    "    split_dt['id'] = f['id'].values\n",
    "    return split_dt, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc57efee-abf4-42fa-9c72-0a3e51a82096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_sample_multilabel(d, var:str, K, seed):\n",
    "    # convert values from list ot sepearte arrays\n",
    "#     d_t = np.array(d[var].apply(lambda x: [int(y) for y in x[1:-1].split(',')]).values.tolist())\n",
    "#     num_cols = d_t.shape[-1]\n",
    "#     columns=['v'+str(i) for i in range(num_cols)]\n",
    "#     split_dt =   pd.DataFrame(np.array(d_t), columns=columns)\n",
    "#     split_dt['id'] = d['id'].values\n",
    "#     d_t = split_dt\n",
    "    d_t, num_cols = split_list_label(d, var)\n",
    "    subs = []\n",
    "    \n",
    "    for i in range(num_cols): \n",
    "        # print(\"col at \", i)\n",
    "        temp = equal_sample(d_t, 'v'+str(i), K, seed=seed)\n",
    "        subs.append(temp)\n",
    "\n",
    "    base = pd.concat(subs, axis=0).drop_duplicates()\n",
    "    num_extra = K*num_cols - base.shape[0]\n",
    "    extra = d_t[~d_t.id.isin(base.id)].sample(num_extra, random_state=seed)\n",
    "    sample_ids = pd.concat([base, extra]).sample(frac=1, random_state=seed)\n",
    "    sample_ids = np.unique(sample_ids['id'])\n",
    "\n",
    "    return d[d.id.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17166c28-3353-44aa-9141-cade55a68c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    def __init__(self, drt:str, filelist:list=['founta'], mainvars:list = ['id', 'name','text_all', 'text_std', 'label_bin', 'label_orig', 'label_target'], \n",
    "                savevars:list = ['id', 'name','text_std',], \n",
    "                label_mode = 'bin', test_ratio=2000, num_pos=1000, pos_ratio=1, val_size=5, max_domain_size=5000):\n",
    "        self.directory = drt + '/' if len(drt) > 0 else ''\n",
    "        self.filelist = list(set(filelist)) #dedup\n",
    "        self.test_ratio = test_ratio\n",
    "        self.num_pos = num_pos\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.val_size = val_size\n",
    "        self.max_domain_size = max_domain_size\n",
    "\n",
    "        self.mainvars = mainvars\n",
    "        self.savevars = deepcopy(savevars)\n",
    "        print(\"CHECK 0\", self.savevars)\n",
    "        self.label_mode = label_mode\n",
    "        self.label_config = dict()\n",
    "        self.report_table = pd.DataFrame()\n",
    "        if self.label_mode == 'bin':\n",
    "            self.savevars += ['label_bin']\n",
    "            self.label_var = 'label_bin'\n",
    "        else:\n",
    "            self.savevars += ['label_orig','label_target']\n",
    "            print(\"CHECK 1\", self.savevars)\n",
    "            if self.label_mode == 'orig':\n",
    "                self.label_var = 'label_orig'\n",
    "            elif self.label_mode == 'target':\n",
    "                self.label_var = 'label_target'\n",
    "            \n",
    "        assert type(self.label_var) is str \n",
    "\n",
    "        self.train_df = pd.DataFrame(columns=self.mainvars)\n",
    "        self.val_df   = pd.DataFrame(columns=self.mainvars)\n",
    "        self.test_df = pd.DataFrame(columns=self.mainvars)\n",
    "        self.test_ids = dict()\n",
    "        self.val_ids  = dict()\n",
    "        \n",
    "        for name in self.filelist: \n",
    "            print(\"Processing \", name)\n",
    "            train,val,test = self.__prepareData__(name)            \n",
    "            # print(\"Shape\", train.shape, val.shape, test.shape)\n",
    "            self.__appendData__(train, 'train')\n",
    "            self.__appendData__(val, 'val')\n",
    "            self.__appendData__(test, 'test')\n",
    "        \n",
    "        # Keep track of ids in val and test data\n",
    "        for k in self.val_df.name.unique():\n",
    "            ids = self.val_df[self.val_df.name == k].id.array.tolist()\n",
    "            self.val_ids[k] = ids \n",
    "        \n",
    "        for k in self.test_df.name.unique():\n",
    "            ids = self.test_df[self.test_df.name == k].id.array.tolist()\n",
    "            self.test_ids[k] = ids     \n",
    "\n",
    "   \n",
    "    def standardize_data(self, segmenter=None):\n",
    "        # standardize data\n",
    "        \n",
    "        if segmenter is not None:\n",
    "            # tokenizer cannot work with NAN data\n",
    "            self.train_df['text_all'].fillna(' ', inplace=True)\n",
    "            self.test_df['text_all'].fillna(' ', inplace=True)\n",
    "\n",
    "            self.train_df['text_std'] = self.train_df.text_all.apply(lambda x: process_tweet_bert(x, dict(), segmenter, verbose=False))\n",
    "            self.test_df['text_std'] = self.test_df.text_all.apply(lambda x: process_tweet_bert(x, dict(), segmenter, verbose=False))\n",
    "        \n",
    "        self.train_df['text_std'].fillna(' ', inplace=True)\n",
    "        self.val_df['text_std'].fillna(' ', inplace=True)\n",
    "        self.test_df['text_std'].fillna(' ', inplace=True)\n",
    "        \n",
    "    \n",
    "    def __appendData__(self, base:pd.DataFrame, to_which='train'):\n",
    "        if to_which == 'train': \n",
    "            self.train_df = pd.concat([base, self.train_df], ignore_index=True)\n",
    "            self.train_df.reset_index(drop=True, inplace=True)\n",
    "        elif to_which == 'val':\n",
    "            self.val_df = pd.concat([base, self.val_df], ignore_index=True)\n",
    "            self.val_df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            self.test_df = pd.concat([base, self.test_df], ignore_index=True)\n",
    "            self.test_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    def __get_problem_config__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Create a dictionary of problem tyes for each variable that contains 'label' in name\n",
    "        \n",
    "        Return: \n",
    "            problem_config: nested dict, {domain: {'label_var': {problem_type, num_classes}}}\n",
    "        \"\"\"\n",
    "        problem_config = dict()\n",
    "        label_vars = [l for l in df.columns if 'label' in l ]\n",
    "        \n",
    "        for label_var in label_vars: \n",
    "            if df[label_var].dtypes == int or df[label_var].dtypes == float:\n",
    "                problem_type = 'single_label_classification'\n",
    "                num_classes = df[label_var].nunique()\n",
    "            elif df[label_var].dtypes == object:\n",
    "                if max(df[label_var].str.len()) == 1:\n",
    "                    problem_type = 'single_label_classification'\n",
    "                    num_classes = df[label_var].nunique()\n",
    "                else: \n",
    "                    problem_type = 'multi_label_classification'\n",
    "                    num_classes = max(df[label_var].str[1:-1].apply(lambda x: len(x.split(','))))\n",
    "\n",
    "            problem_config[label_var] = {'problem_type': problem_type,  'num_classes': num_classes}\n",
    "        \n",
    "        return problem_config \n",
    "            \n",
    "        \n",
    "    def __prepareData__(self, name, sep=','): \n",
    "        \"\"\"\n",
    "        Split dataframe into processed train and original test \n",
    "        @param fname: name of directory that contains dataset\n",
    "        @param test_ratio: float, ratio of test size on origial shape \n",
    "        @param num_pos: int, OVERLOADED if label_mode = bin, number of positive samples to be included in train set\n",
    "                        else, it is the number of samples to be included in train set \n",
    "        @param pos_ratio: float, ratio of pos vs neg samples\n",
    "        \"\"\" \n",
    "        fname = name[0].lower() + name[1:]\n",
    "        dt = pd.read_csv(self.directory + '/' + fname + '.csv', delimiter=sep)\n",
    "        dt['name'] = fname\n",
    "        dt = dt.sample(frac=1, random_state=seed) # shuffle first\n",
    "        \n",
    "        problem_config =  self.__get_problem_config__(dt)\n",
    "        self.label_config[fname] = problem_config\n",
    "        \n",
    "        dt = dt[[var for var in self.mainvars if var in dt.columns]]\n",
    "        if  self.label_mode == 'bin' and  len(dt.label_bin.unique()) > 3: \n",
    "            print(name, '\\n', dt.label_bin.unique()) \n",
    "            print(\"Before\\n\",train.label_bin.unique(), test.label_bin.unique(), '\\n')\n",
    "        \n",
    "        # val and test set are not artificially balanced \n",
    "        train, test = train_test_split(dt, test_size=self.test_ratio, shuffle=False)\n",
    "        train, val  = train_test_split(train, test_size=self.val_size, shuffle=False)\n",
    "        val = val.sample(frac=1, random_state=seed)\n",
    "        \n",
    "        # equal samples of the chosen variable if possible\n",
    "        if self.label_mode in ['bin', 'orig', 'target']:\n",
    "            label_var = self.label_var\n",
    "            if self.label_var not in dt.columns and self.label_mode != 'bin':\n",
    "                label_var = 'label_orig'\n",
    "            if problem_config[label_var]['problem_type'] == 'multi_label_classification':\n",
    "                train = equal_sample_multilabel(train, label_var, self.num_pos, seed)\n",
    "            else:\n",
    "                K_required = self.num_pos * dt[label_var].nunique()\n",
    "                print(\"K_required: {}\".format(K_required))\n",
    "                train = equal_sample(train, label_var, K_required, seed)\n",
    "            \n",
    "            # enforce maximum limit per set\n",
    "            if train.shape[0] > self.max_domain_size: \n",
    "                train = train.sample(self.max_domain_size, random_state=seed)\n",
    "        else: \n",
    "            train = train.sample(self.num_pos, random_state=seed)\n",
    "        return train, val, test \n",
    "    \n",
    "    \n",
    "    def report(self, print_dir = None ):\n",
    "        label_var = 'label_orig' if self.label_mode == 'target' else self.label_var\n",
    "        report_vars = ['name'] if self.label_mode == 'bin' else ['name', label_var]\n",
    "        train = self.train_df.groupby(report_vars)[label_var].count()\n",
    "        test = self.test_df.groupby(report_vars)[label_var].count()\n",
    "        report = pd.concat([train, test], axis=1)\n",
    "        report.columns = ['Train','Test']\n",
    "        self.report_table = report\n",
    "        print(\"Total number of datasets:\", report.shape[0],\"\\n\", report)\n",
    "        print(\"TEXT LENGTHS\\nTrain set:\\n\",pd.DataFrame(self.train_df.text_all.str.len()).describe())\n",
    "        print(\"Test set:\\n\",pd.DataFrame(self.test_df.text_std.str.len()).describe())\n",
    "\n",
    "        \n",
    "    def saveFiles(self, save_dir:str, fname_prefix:str, save_test = True, sep=','):\n",
    "        # save label_config\n",
    "        label_config_dir = save_dir + '/' + fname_prefix + '_' + str(len(self.filelist)) \n",
    "        with open(label_config_dir + '_label_config.json','w') as l :\n",
    "                json.dump(self.label_config, l)\n",
    "        l.close()\n",
    "        \n",
    "        train_dir = save_dir + '/' + fname_prefix + '_' + str(len(self.filelist)) + '_' + str(self.num_pos) +  '_' + \\\n",
    "                 str(self.test_ratio)  + '_' + self.label_mode\n",
    "        test_dir  = save_dir + '/' + fname_prefix + '_' + str(len(self.filelist)) + '_' + str(self.test_ratio) + '_' + self.label_mode\n",
    "        self.train_df[self.savevars].to_csv(train_dir + '_train.csv', index=None, sep=sep)\n",
    "        print(\"CHECK\", self.savevars )\n",
    "        \n",
    "        if save_test: \n",
    "            self.test_df[self.savevars].to_csv(test_dir + '_test.csv', index=None, sep=sep)\n",
    "            with open(test_dir + '_testids.json','w') as f :\n",
    "                json.dump(self.test_ids, f)\n",
    "            f.close()\n",
    "            \n",
    "            self.val_df[self.savevars].to_csv(test_dir + '_val.csv', index=None, sep=sep)\n",
    "            with open(test_dir + '_valids.json','w') as g :\n",
    "                json.dump(self.val_ids, g)\n",
    "            g.close()\n",
    "                \n",
    "        self.report()\n",
    "        if self.label_mode != 'bin':\n",
    "            self.report_table.to_csv(test_dir + '_report.csv', sep=sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7214ce-740b-418e-821e-cca2eecf994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 0 ['id', 'name', 'text_std']\n",
      "Processing  Gab_kennedy\n",
      "K_required: 4000\n",
      "Processing  Jigsaw\n",
      "K_required: 4000\n",
      "Processing  Olid\n",
      "K_required: 4000\n",
      "Processing  Waseem\n",
      "K_required: 4000\n",
      "Processing  Sab\n",
      "K_required: 4000\n",
      "Processing  Goldbeck\n",
      "K_required: 4000\n",
      "Processing  Davidson\n",
      "K_required: 4000\n",
      "Processing  Hateval\n",
      "K_required: 4000\n",
      "Processing  Trac\n",
      "K_required: 4000\n",
      "Processing  Founta\n",
      "K_required: 4000\n",
      "CHECK ['id', 'name', 'text_std', 'label_bin']\n",
      "Total number of datasets: 10 \n",
      "              Train  Test\n",
      "name                    \n",
      "davidson      4000  2000\n",
      "founta        4000  2000\n",
      "gab_kennedy   4000  2000\n",
      "goldbeck      4000  2000\n",
      "hateval       4000  2000\n",
      "jigsaw        4000  2000\n",
      "olid          4000  2000\n",
      "sab           4000  2000\n",
      "trac          4000  2000\n",
      "waseem        4000  2000\n",
      "TEXT LENGTHS\n",
      "Train set:\n",
      "            text_all\n",
      "count  39997.000000\n",
      "mean     141.077381\n",
      "std      217.176073\n",
      "min        2.000000\n",
      "25%       68.000000\n",
      "50%      110.000000\n",
      "75%      141.000000\n",
      "max     5000.000000\n",
      "Test set:\n",
      "            text_std\n",
      "count  20000.000000\n",
      "mean     133.703100\n",
      "std      227.262826\n",
      "min        2.000000\n",
      "25%       61.000000\n",
      "50%       97.000000\n",
      "75%      133.000000\n",
      "max     5000.000000\n"
     ]
    }
   ],
   "source": [
    "filelist = ['Waseem', 'Davidson', 'Trac', 'Hateval','Jigsaw', 'Olid', 'Founta','Goldbeck', 'Gab_kennedy', 'Sab']\n",
    "# filelist = ['sab']\n",
    "test_ratio=2000\n",
    "num_poses = [2000]\n",
    "label_mode = 'bin'\n",
    "\n",
    "for i, num_pos in enumerate(num_poses):\n",
    "    b = DataCollector(drt='Meta_data/Raw', test_ratio=test_ratio, filelist=filelist, num_pos=num_pos, label_mode=label_mode, \n",
    "                      savevars= ['id', 'name','text_std'])\n",
    "    b.standardize_data()\n",
    "#     print(o.train_df.label_bin.unique())\n",
    "#     print(o.test_df.label_bin.unique())\n",
    "    if i == 0: \n",
    "        b.saveFiles('Meta_data', 'meta')\n",
    "    else:\n",
    "        b.saveFiles('Meta_data', 'meta', save_test = False)\n",
    "    del b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86ac9bd-4cdc-4d80-9bd9-bdb1f1f079b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 0 ['id', 'name', 'text_std']\n",
      "CHECK 1 ['id', 'name', 'text_std', 'label_orig', 'label_target']\n",
      "Processing  Gab_kennedy\n",
      "Processing  Jigsaw\n",
      "Processing  Olid\n",
      "K_required: 3000\n",
      "Processing  Waseem\n",
      "K_required: 3000\n",
      "Processing  Sab\n",
      "K_required: 2000\n",
      "Processing  Goldbeck\n",
      "K_required: 2000\n",
      "Processing  Davidson\n",
      "K_required: 3000\n",
      "Processing  Hateval\n",
      "K_required: 3000\n",
      "Processing  Trac\n",
      "K_required: 3000\n",
      "Processing  Founta\n",
      "K_required: 4000\n",
      "CHECK ['id', 'name', 'text_std', 'label_orig', 'label_target']\n",
      "Total number of datasets: 72 \n",
      "                      Train    Test\n",
      "name     label_orig               \n",
      "davidson 0            1000   127.0\n",
      "         1            1000  1517.0\n",
      "         2            1000   356.0\n",
      "founta   0            1000   306.0\n",
      "         1            1000  1053.0\n",
      "...                    ...     ...\n",
      "trac     1            1000   662.0\n",
      "         2            1000   452.0\n",
      "waseem   0            1496  1466.0\n",
      "         1            1495   531.0\n",
      "         2               9     3.0\n",
      "\n",
      "[72 rows x 2 columns]\n",
      "TEXT LENGTHS\n",
      "Train set:\n",
      "            text_all\n",
      "count  30997.000000\n",
      "mean     158.235281\n",
      "std      300.855984\n",
      "min        2.000000\n",
      "25%       69.000000\n",
      "50%      112.000000\n",
      "75%      143.000000\n",
      "max     6978.000000\n",
      "Test set:\n",
      "            text_std\n",
      "count  20000.000000\n",
      "mean     133.703100\n",
      "std      227.262826\n",
      "min        2.000000\n",
      "25%       61.000000\n",
      "50%       97.000000\n",
      "75%      133.000000\n",
      "max     5000.000000\n"
     ]
    }
   ],
   "source": [
    "test_ratio=2000\n",
    "num_poses = [1000] # for each class of the specified label\n",
    "label_mode = 'orig'\n",
    "for i, num_pos in enumerate(num_poses):\n",
    "    o = DataCollector(drt='Meta_data/Raw', test_ratio=test_ratio, filelist=filelist, num_pos=num_pos, label_mode=label_mode)\n",
    "    o.standardize_data()\n",
    "    if i == 0: \n",
    "        o.saveFiles('Meta_data', 'meta')\n",
    "    else:\n",
    "        o.saveFiles('Meta_data', 'meta', save_test = False)\n",
    "    del o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217de2bc-6037-4733-bcce-90b7b3739460",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## END OF OFFICIAL CODE ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a08bca-21aa-4251-a9dd-556198332822",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DO NOT DELETE ##############\n",
    "## TEST EQUAL_SAMPLE_MULTILABLE #####3\n",
    "name = 'gab_kennedy'\n",
    "mainvars = ['id', 'name','text_all', 'text_std', 'label_bin', 'label_orig', 'label_target']\n",
    "# os.listdir('Meta_data/Raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1f7d5-a422-402d-8504-9adf0ee01a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('Meta_data/Raw/'+name+'.csv')\n",
    "d['name'] = name\n",
    "label_var = 'label_orig' if 'label_orig' in d.columns else 'label_target'\n",
    "d = d.sample(frac=1, random_state=seed) # shuffle first\n",
    "d = d[[var for var in mainvars if var in d.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10d05f-1c2a-4b4f-9469-0964a7cd3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test split\n",
    "x, y = split_list_label(d, label_var)\n",
    "x.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e2b76-8a42-4299-9a12-2e484c2e7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = d[:5000]\n",
    "a = equal_sample_multilabel(train, 'label_orig', 100, 123)\n",
    "select_ids = a.id.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe945a08-33d1-4a99-a103-0c0817d68b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels values do not change\n",
    "assert all(a[a.id.isin(select_ids)].label_orig.values == d[d.id.isin(select_ids)].label_orig.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecba19-c77e-4d2f-b975-abee99661603",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _ = split_list_label(a, label_var)\n",
    "b.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba7673-29d0-4611-ae52-03bb923837d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3d8e1-1fae-4772-a77e-bf17f8481c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def equal_sampling(df, k, var, seed=123):\n",
    "#     counter_dict = dict(Counter(df[var]))\n",
    "#     print(counter_dict)\n",
    "#     num_samples = int(k / len(counter_dict))\n",
    "#     stratas = []\n",
    "    \n",
    "#     for c, count in counter_dict.items():\n",
    "#         strata = df[df[var] == c].sample(min(num_samples,count), random_state=123)\n",
    "#         stratas.append(strata)\n",
    "    \n",
    "#     sample = pd.concat(stratas)\n",
    "#     print(\"Sample shape \", sample.shape)\n",
    "#     # in case there is not enough samples to meet quota, backfill \n",
    "#     if sample.shape[0] < k:\n",
    "#         complement = df.iloc[~df.index.isin(sample.index)]\n",
    "        \n",
    "#     return complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a8f1e-e2dc-4ede-bc6d-77c335e6c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_csv(\"Meta_data/Raw/founta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b78df3-4c75-4f1d-9683-effce548eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = equal_sampling(temp, 500, 'label_orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e60416-fbb1-48eb-9903-4808aee38e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a32d0-984a-40c2-ac3b-87f82176da94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9691d-972e-42c1-9f54-c586d8739252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_hate",
   "language": "python",
   "name": "covid_hate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
